{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e63a7cd-73c6-42c2-abe0-6451f99e6f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\SIH\\AI\\Branched\\Branched/Circumferential Crack\n",
      "E:\\SIH\\AI\\Branched\\Branched/Gasket Leak\n",
      "E:\\SIH\\AI\\Branched\\Branched/Longitudinal Crack\n",
      "E:\\SIH\\AI\\Branched\\Branched/NonLeak\n",
      "E:\\SIH\\AI\\Branched\\Branched/Orifice Leak\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the relative path to your files (adjust this path as needed)\n",
    "relative_path = 'Branched/'\n",
    "\n",
    "# Combine the current directory with the relative path to get the full path\n",
    "full_path = os.path.join(current_directory, relative_path)\n",
    "\n",
    "# List all files in the directory\n",
    "for filename in os.listdir(full_path):\n",
    "    print(os.path.join(full_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29c0701-b0ae-4dfc-8fa2-44f6e8f799b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LogaSanjeev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89697e72-d4fb-4c54-bc01-59d5d00cb914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Value1    Value2               LeakType\n",
      "0       0.015076 -0.000970  Circumferential Crack\n",
      "1       0.009311 -0.000620  Circumferential Crack\n",
      "2       0.004548 -0.000458  Circumferential Crack\n",
      "3       0.000521 -0.000074  Circumferential Crack\n",
      "4      -0.006283 -0.000187  Circumferential Crack\n",
      "...          ...       ...                    ...\n",
      "936562       NaN  0.000501  Circumferential Crack\n",
      "936563       NaN  0.000395  Circumferential Crack\n",
      "936564       NaN  0.000183  Circumferential Crack\n",
      "936565       NaN  0.000120  Circumferential Crack\n",
      "936566       NaN -0.000075  Circumferential Crack\n",
      "\n",
      "[936567 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "csv_directory = 'Branched/Circumferential Crack/'\n",
    "\n",
    "# List all files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory) if file.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read them into DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df['Value'])\n",
    "    #print(\"Dataframe 1\",file_path,dataframes)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dataframes, axis=1, ignore_index=True)\n",
    "\n",
    "# Add leaktype\n",
    "df[\"LeakType\"] = \"Circumferential Crack\"\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={0: \"Value1\", 1: \"Value2\"})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6869f820-6aa4-419b-a844-cc051988174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Value1    Value2     LeakType\n",
      "0       0.000554 -0.002314  Gasket Leak\n",
      "1       0.000301 -0.002494  Gasket Leak\n",
      "2       0.002993 -0.002450  Gasket Leak\n",
      "3       0.007914 -0.002386  Gasket Leak\n",
      "4       0.007360 -0.002521  Gasket Leak\n",
      "...          ...       ...          ...\n",
      "928285  0.000273       NaN  Gasket Leak\n",
      "928286  0.000965       NaN  Gasket Leak\n",
      "928287  0.000244       NaN  Gasket Leak\n",
      "928288  0.001058       NaN  Gasket Leak\n",
      "928289  0.000121       NaN  Gasket Leak\n",
      "\n",
      "[928290 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "csv_directory1 = 'Branched/Gasket Leak/'\n",
    "\n",
    "# List all files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory1) if file.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read them into DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_directory1, file)\n",
    "    df1 = pd.read_csv(file_path)\n",
    "    dataframes.append(df1['Value'])\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df1 = pd.concat(dataframes, axis=1, ignore_index=True)\n",
    "\n",
    "# Add leaktype\n",
    "df1[\"LeakType\"] = \"Gasket Leak\"\n",
    "\n",
    "# Rename columns\n",
    "df1 = df1.rename(columns={0: \"Value1\", 1: \"Value2\"})\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82917b26-fb7e-4e00-8914-4b0543d225a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Value1    Value2            LeakType\n",
      "0        0.017202  0.002530  Longitudinal Crack\n",
      "1        0.016271  0.002150  Longitudinal Crack\n",
      "2        0.016820  0.002151  Longitudinal Crack\n",
      "3        0.015964  0.002196  Longitudinal Crack\n",
      "4        0.013826  0.002152  Longitudinal Crack\n",
      "...           ...       ...                 ...\n",
      "1048567  0.001603       NaN  Longitudinal Crack\n",
      "1048568  0.001454       NaN  Longitudinal Crack\n",
      "1048569  0.001405       NaN  Longitudinal Crack\n",
      "1048570  0.001308       NaN  Longitudinal Crack\n",
      "1048571  0.001115       NaN  Longitudinal Crack\n",
      "\n",
      "[1048572 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "csv_directory2 = 'Branched/Longitudinal Crack'\n",
    "\n",
    "# List all files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory2) if file.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read them into DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_directory2, file)\n",
    "    df2 = pd.read_csv(file_path)\n",
    "    dataframes.append(df2['Value'])\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df2 = pd.concat(dataframes, axis=1, ignore_index=True)\n",
    "\n",
    "# Add leaktype\n",
    "df2[\"LeakType\"] = \"Longitudinal Crack\"\n",
    "\n",
    "# Rename columns\n",
    "df2 = df2.rename(columns={0: \"Value1\", 1: \"Value2\"})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5211e8-6a0d-433d-bef4-225d9ca56f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Value1    Value2 LeakType\n",
      "0       0.024519  0.001431  NonLeak\n",
      "1       0.031200  0.001604  NonLeak\n",
      "2       0.031983  0.001683  NonLeak\n",
      "3       0.030525  0.001769  NonLeak\n",
      "4       0.032294  0.001783  NonLeak\n",
      "...          ...       ...      ...\n",
      "937068       NaN  0.004447  NonLeak\n",
      "937069       NaN  0.004422  NonLeak\n",
      "937070       NaN  0.004404  NonLeak\n",
      "937071       NaN  0.004424  NonLeak\n",
      "937072       NaN  0.004226  NonLeak\n",
      "\n",
      "[937073 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "csv_directory3 = \"Branched/NonLeak\"\n",
    "\n",
    "# List all files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory3) if file.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read them into DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_directory3, file)\n",
    "    df3 = pd.read_csv(file_path)\n",
    "    dataframes.append(df3['Value'])\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df3 = pd.concat(dataframes, axis=1, ignore_index=True)\n",
    "\n",
    "# Add leaktype\n",
    "df3[\"LeakType\"] = \"NonLeak\"\n",
    "\n",
    "# Rename columns\n",
    "df3 = df3.rename(columns={0: \"Value1\", 1: \"Value2\"})\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7209d45f-bce4-416f-acb8-de231d955b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Value1    Value2      LeakType\n",
      "0       0.017957 -0.001238  Orifice Leak\n",
      "1       0.025365 -0.001147  Orifice Leak\n",
      "2       0.026388 -0.000925  Orifice Leak\n",
      "3       0.025678 -0.000857  Orifice Leak\n",
      "4       0.025522 -0.000955  Orifice Leak\n",
      "...          ...       ...           ...\n",
      "960679       NaN  0.000960  Orifice Leak\n",
      "960680       NaN  0.000793  Orifice Leak\n",
      "960681       NaN  0.000600  Orifice Leak\n",
      "960682       NaN  0.000621  Orifice Leak\n",
      "960683       NaN  0.000482  Orifice Leak\n",
      "\n",
      "[960684 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "csv_directory4 = \"Branched/Orifice Leak\"\n",
    "\n",
    "# List all files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory4) if file.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read them into DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(csv_directory4, file)\n",
    "    df4 = pd.read_csv(file_path)\n",
    "    dataframes.append(df4['Value'])\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df4 = pd.concat(dataframes, axis=1, ignore_index=True)\n",
    "\n",
    "# Add leaktype\n",
    "df4[\"LeakType\"] = \"Orifice Leak\"\n",
    "\n",
    "# Rename columns\n",
    "df4 = df4.rename(columns={0: \"Value1\", 1: \"Value2\"})\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d4ee6a-a7de-4c42-ba04-c385bcaf865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value1</th>\n",
       "      <th>Value2</th>\n",
       "      <th>LeakType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015076</td>\n",
       "      <td>-0.000970</td>\n",
       "      <td>Circumferential Crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009311</td>\n",
       "      <td>-0.000620</td>\n",
       "      <td>Circumferential Crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004548</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>Circumferential Crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000521</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>Circumferential Crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.006283</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>Circumferential Crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960679</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960680</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960681</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960682</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960683</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4811186 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Value1    Value2               LeakType\n",
       "0       0.015076 -0.000970  Circumferential Crack\n",
       "1       0.009311 -0.000620  Circumferential Crack\n",
       "2       0.004548 -0.000458  Circumferential Crack\n",
       "3       0.000521 -0.000074  Circumferential Crack\n",
       "4      -0.006283 -0.000187  Circumferential Crack\n",
       "...          ...       ...                    ...\n",
       "960679       NaN  0.000960           Orifice Leak\n",
       "960680       NaN  0.000793           Orifice Leak\n",
       "960681       NaN  0.000600           Orifice Leak\n",
       "960682       NaN  0.000621           Orifice Leak\n",
       "960683       NaN  0.000482           Orifice Leak\n",
       "\n",
       "[4811186 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([df, df1, df2, df3, df4], axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5afd15-808d-4717-bdbf-c3be87d9eac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value1</th>\n",
       "      <th>Value2</th>\n",
       "      <th>LeakType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>877088</th>\n",
       "      <td>0.001558</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>Gasket Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497860</th>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32815</th>\n",
       "      <td>-0.015997</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>Longitudinal Crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754474</th>\n",
       "      <td>-0.000638</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>NonLeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227346</th>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>NonLeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230288</th>\n",
       "      <td>0.003256</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>NonLeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747231</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503110</th>\n",
       "      <td>0.004113</td>\n",
       "      <td>-0.000858</td>\n",
       "      <td>Gasket Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194897</th>\n",
       "      <td>0.030971</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>Gasket Leak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241697</th>\n",
       "      <td>0.016675</td>\n",
       "      <td>-0.002133</td>\n",
       "      <td>Orifice Leak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4811186 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Value1    Value2            LeakType\n",
       "877088  0.001558 -0.003894         Gasket Leak\n",
       "497860  0.022819  0.003290        Orifice Leak\n",
       "32815  -0.015997  0.000073  Longitudinal Crack\n",
       "754474 -0.000638  0.002956             NonLeak\n",
       "227346  0.003482  0.005195             NonLeak\n",
       "...          ...       ...                 ...\n",
       "230288  0.003256 -0.000072             NonLeak\n",
       "747231  0.000631  0.001080        Orifice Leak\n",
       "503110  0.004113 -0.000858         Gasket Leak\n",
       "194897  0.030971 -0.001396         Gasket Leak\n",
       "241697  0.016675 -0.002133        Orifice Leak\n",
       "\n",
       "[4811186 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= data.sample(frac=1,random_state=55)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6866b44e-da97-4710-932f-ea87261ef943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gasket Leak', 'Orifice Leak', 'Longitudinal Crack', 'NonLeak',\n",
       "       'Circumferential Crack'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['LeakType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb05bbe-a94b-414d-865b-171b07af9c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value1</th>\n",
       "      <th>Value2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.803093e+06</td>\n",
       "      <td>4.675945e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.555970e-04</td>\n",
       "      <td>2.110810e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.148928e-02</td>\n",
       "      <td>2.596954e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.743199e-01</td>\n",
       "      <td>-2.162147e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.814955e-03</td>\n",
       "      <td>-1.263028e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.762470e-04</td>\n",
       "      <td>2.779270e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.104115e-03</td>\n",
       "      <td>1.780965e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.474576e-01</td>\n",
       "      <td>2.381822e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Value1        Value2\n",
       "count  4.803093e+06  4.675945e+06\n",
       "mean   1.555970e-04  2.110810e-04\n",
       "std    1.148928e-02  2.596954e-03\n",
       "min   -1.743199e-01 -2.162147e-01\n",
       "25%   -3.814955e-03 -1.263028e-03\n",
       "50%    1.762470e-04  2.779270e-04\n",
       "75%    4.104115e-03  1.780965e-03\n",
       "max    1.474576e-01  2.381822e-01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f74a21-0b08-4545-889f-e561f4964f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['category_label_encoded'] = label_encoder.fit_transform(data['LeakType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62d0efb-9de0-4474-8bfa-cd2d7fea5294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Value1        Value2  category_label_encoded\n",
      "count  4.667852e+06  4.667852e+06            4.667852e+06\n",
      "mean   1.563120e-04  2.106782e-04            2.020782e+00\n",
      "std    1.165364e-02  2.598192e-03            1.418662e+00\n",
      "min   -1.743199e-01 -2.162147e-01            0.000000e+00\n",
      "25%   -4.206906e-03 -1.264213e-03            1.000000e+00\n",
      "50%    1.792340e-04  2.776310e-04            2.000000e+00\n",
      "75%    4.491883e-03  1.781558e-03            3.000000e+00\n",
      "max    1.474576e-01  2.381822e-01            4.000000e+00\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9a9694-a656-42af-8f22-da13acada1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value1                    144631\n",
       "Value2                     39973\n",
       "LeakType                       5\n",
       "category_label_encoded         5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a2e197-1322-4c85-b4ca-b1d62b767c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value1</th>\n",
       "      <th>Value2</th>\n",
       "      <th>LeakType</th>\n",
       "      <th>category_label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>877088</th>\n",
       "      <td>0.001558</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>Gasket Leak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497860</th>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>Orifice Leak</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32815</th>\n",
       "      <td>-0.015997</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>Longitudinal Crack</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754474</th>\n",
       "      <td>-0.000638</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>NonLeak</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227346</th>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>NonLeak</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230288</th>\n",
       "      <td>0.003256</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>NonLeak</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747231</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>Orifice Leak</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503110</th>\n",
       "      <td>0.004113</td>\n",
       "      <td>-0.000858</td>\n",
       "      <td>Gasket Leak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194897</th>\n",
       "      <td>0.030971</td>\n",
       "      <td>-0.001396</td>\n",
       "      <td>Gasket Leak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241697</th>\n",
       "      <td>0.016675</td>\n",
       "      <td>-0.002133</td>\n",
       "      <td>Orifice Leak</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4661521 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Value1    Value2            LeakType  category_label_encoded\n",
       "877088  0.001558 -0.003894         Gasket Leak                       1\n",
       "497860  0.022819  0.003290        Orifice Leak                       4\n",
       "32815  -0.015997  0.000073  Longitudinal Crack                       2\n",
       "754474 -0.000638  0.002956             NonLeak                       3\n",
       "227346  0.003482  0.005195             NonLeak                       3\n",
       "...          ...       ...                 ...                     ...\n",
       "230288  0.003256 -0.000072             NonLeak                       3\n",
       "747231  0.000631  0.001080        Orifice Leak                       4\n",
       "503110  0.004113 -0.000858         Gasket Leak                       1\n",
       "194897  0.030971 -0.001396         Gasket Leak                       1\n",
       "241697  0.016675 -0.002133        Orifice Leak                       4\n",
       "\n",
       "[4661521 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e1c877-92c5-4571-aae3-9a282687032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value1</th>\n",
       "      <th>Value2</th>\n",
       "      <th>category_label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.667852e+06</td>\n",
       "      <td>4.667852e+06</td>\n",
       "      <td>4.667852e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.563120e-04</td>\n",
       "      <td>2.106782e-04</td>\n",
       "      <td>2.020782e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.165364e-02</td>\n",
       "      <td>2.598192e-03</td>\n",
       "      <td>1.418662e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.743199e-01</td>\n",
       "      <td>-2.162147e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.206906e-03</td>\n",
       "      <td>-1.264213e-03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.792340e-04</td>\n",
       "      <td>2.776310e-04</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.491883e-03</td>\n",
       "      <td>1.781558e-03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.474576e-01</td>\n",
       "      <td>2.381822e-01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Value1        Value2  category_label_encoded\n",
       "count  4.667852e+06  4.667852e+06            4.667852e+06\n",
       "mean   1.563120e-04  2.106782e-04            2.020782e+00\n",
       "std    1.165364e-02  2.598192e-03            1.418662e+00\n",
       "min   -1.743199e-01 -2.162147e-01            0.000000e+00\n",
       "25%   -4.206906e-03 -1.264213e-03            1.000000e+00\n",
       "50%    1.792340e-04  2.776310e-04            2.000000e+00\n",
       "75%    4.491883e-03  1.781558e-03            3.000000e+00\n",
       "max    1.474576e-01  2.381822e-01            4.000000e+00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e3148b-4dc8-4e49-97c0-468526a71e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4667852, 4)\n",
      "Data without outliers shape: (4570284, 4)\n",
      "Missing values after removing outliers:\n",
      "Value1                    0\n",
      "Value2                    0\n",
      "LeakType                  0\n",
      "category_label_encoded    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "z_scores = data.select_dtypes(include=['float64']).apply(zscore)\n",
    "\n",
    "# Define a threshold for identifying outliers (adjust as needed)\n",
    "threshold = 3\n",
    "\n",
    "# Identify and remove outliers\n",
    "data_no_outliers = data[(z_scores.abs() < threshold).all(axis=1)]\n",
    "\n",
    "# Compare shapes to see how many outliers were removed\n",
    "print(\"Original shape:\", data.shape)\n",
    "print(\"Data without outliers shape:\", data_no_outliers.shape)\n",
    "\n",
    "# Check for missing values after removing outliers\n",
    "missing_values_after = data_no_outliers.isna().sum()\n",
    "print(\"Missing values after removing outliers:\")\n",
    "print(missing_values_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b38e6b0e-1a55-4859-beaa-485e63dac9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Value1        Value2  LeakType  category_label_encoded\n",
      "count  4.667852e+06  4.667852e+06       0.0            4.667852e+06\n",
      "mean   1.563120e-04  2.106782e-04       NaN            2.020782e+00\n",
      "std    1.165364e-02  2.598192e-03       NaN            1.418662e+00\n",
      "min   -1.743199e-01 -2.162147e-01       NaN            0.000000e+00\n",
      "10%   -1.433667e-02 -2.850786e-03       NaN            0.000000e+00\n",
      "25%   -4.206906e-03 -1.264213e-03       NaN            1.000000e+00\n",
      "50%    1.792340e-04  2.776310e-04       NaN            2.000000e+00\n",
      "75%    4.491883e-03  1.781558e-03       NaN            3.000000e+00\n",
      "90%    1.462285e-02  3.225944e-03       NaN            4.000000e+00\n",
      "max    1.474576e-01  2.381822e-01       NaN            4.000000e+00\n",
      "Value1                   -0.064588\n",
      "Value2                   -0.199710\n",
      "LeakType                       NaN\n",
      "category_label_encoded   -0.022756\n",
      "dtype: float64\n",
      "Value1                     3.934268\n",
      "Value2                    46.753252\n",
      "LeakType                        NaN\n",
      "category_label_encoded    -1.303683\n",
      "dtype: float64\n",
      "                          Value1    Value2  LeakType  category_label_encoded\n",
      "Value1                  1.000000  0.000424       NaN               -0.001213\n",
      "Value2                  0.000424  1.000000       NaN               -0.000936\n",
      "LeakType                     NaN       NaN       NaN                     NaN\n",
      "category_label_encoded -0.001213 -0.000936       NaN                1.000000\n",
      "Value1                          0\n",
      "Value2                          0\n",
      "LeakType                  4667852\n",
      "category_label_encoded          0\n",
      "dtype: int64\n",
      "Value1                      0.0\n",
      "Value2                      0.0\n",
      "LeakType                  100.0\n",
      "category_label_encoded      0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Handle non-numeric values\n",
    "data_numeric = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary_stats = data_numeric.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "skewness = data_numeric.skew()\n",
    "kurtosis = data_numeric.kurtosis()\n",
    "correlation_matrix = data_numeric.corr()\n",
    "\n",
    "# Count missing values\n",
    "missing_values = data_numeric.isnull().sum()\n",
    "missing_percent = (missing_values / len(data_numeric)) * 100\n",
    "\n",
    "# Print or analyze the calculated statistics as needed\n",
    "print(summary_stats)\n",
    "print(skewness)\n",
    "print(kurtosis)\n",
    "print(correlation_matrix)\n",
    "print(missing_values)\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9a878e7-705f-4224-bc49-240a792b07c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3734281, 2)\n",
      "(933571, 2)\n",
      "(3734281,)\n",
      "(933571,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already preprocessed your data and split it into X and y\n",
    "X = data.drop(['LeakType', 'category_label_encoded'], axis=1)  # Features\n",
    "y = data['category_label_encoded']  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b209765b-1303-43b6-9695-77a5eb483f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db233b57-66be-4a68-a972-49993095dce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LogaSanjeev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\LogaSanjeev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\LogaSanjeev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\LogaSanjeev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "233393/233393 [==============================] - 199s 849us/step - loss: 1.5531 - accuracy: 0.2733 - val_loss: 1.5510 - val_accuracy: 0.2769\n",
      "Epoch 2/2\n",
      "233393/233393 [==============================] - 197s 845us/step - loss: 1.5498 - accuracy: 0.2765 - val_loss: 1.5520 - val_accuracy: 0.2758\n",
      "29175/29175 [==============================] - 19s 645us/step - loss: 1.5520 - accuracy: 0.2758\n",
      "Test Loss: 1.5520\n",
      "Test Accuracy: 0.2758\n",
      "29175/29175 [==============================] - 17s 585us/step\n"
     ]
    }
   ],
   "source": [
    "# # Build DNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))  # Output layer, 5 for the number of unique categories\n",
    "\n",
    "# # Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2, batch_size=16, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# # Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# # Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# # Optionally, you can convert the predicted probabilities to class labels\n",
    "predicted_labels = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41cd22b2-83a3-4bfa-86d7-e5ceb31dbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Leak_Prediction.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c30199e-d067-4a27-bba9-15b7ab2851ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the saved file\n",
    "loaded_model = load_model('Leak_Prediction.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8a048c2-612e-45da-b240-a004636af845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [acceleration, acceleration]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Preprocess the new data (scaling)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 44\u001b[0m new_data_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     47\u001b[0m predictions \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mpredict(new_data_scaled)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:839\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:875\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    874\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 875\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    974\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import mysql.connector\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Connect to MySQL database\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    "    database=\"sensor_data\"\n",
    ")\n",
    "\n",
    "# Function to fetch data from MySQL\n",
    "def fetch_data(table_name, columns):\n",
    "    cursor = db_connection.cursor()\n",
    "    query = f\"SELECT {', '.join(columns)} FROM {table_name} LIMIT 60;\"\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Define columns for acceleration data\n",
    "columns_noleak = ['acceleration']\n",
    "columns_leak = ['acceleration']\n",
    "\n",
    "# Fetch data from MySQL for 'randomdata_sensor_noleak' and 'randomdata_sensor_leak' tables\n",
    "data_noleak = fetch_data('randomdata_sensor', columns_noleak)\n",
    "data_leak = fetch_data('randomdata_sensor_noleak', columns_leak)\n",
    "\n",
    "\n",
    "new_data = pd.concat([data_noleak, data_leak], axis=1)\n",
    "print(new_data)\n",
    "# Load the pre-trained model\n",
    "loaded_model = load_model('Leak_Prediction.keras')  # Replace 'my_model.keras' with the actual file name\n",
    "\n",
    "\n",
    "# Preprocess the new data (scaling)\n",
    "scaler = StandardScaler()\n",
    "new_data_scaled = scaler.fit_transform(new_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(new_data_scaled)\n",
    "\n",
    "# Get the predicted class with the highest probability for each sample\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "\n",
    "#creating counter object\n",
    "count= Counter(predicted_labels)\n",
    "#maximume number of occurences\n",
    "max_count = max(count.most_common(), key=lambda x: x[1])\n",
    "\n",
    "if(max_count[0]==0):\n",
    "    print(\"Predicted Leak Type: Circumfrential Crack\")\n",
    "elif(max_count[0]==1):\n",
    "    print(\"Predicted Leak Type: Gasket Leak\")\n",
    "elif(max_count[0]==2):\n",
    "    print(\"Predicted Leak Type: Longitudinal Crack\")\n",
    "elif(max_count[0]==3):\n",
    "    print(\"Predicted Leak Type: Non Leak\")\n",
    "else:\n",
    "    print(\"Predicted Leak Type: Orifice Leak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71034584-b565-4e55-9923-2da33526417f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
